# IsentiaChallenge_ec2.yml

# This playbook requires an AWS profile on the local machine and an EC2 key
# ansible-playbook IsentiaChallenge_ec2-.yml --key-file "~/.aws/dev_staging.pem"

# On my local machine I'm using python 3.5 with boto 2.49.0 / botocore 1.4.70.  But boto keeps failing with 
#    fatal: [localhost]: FAILED! => {"changed": false, "failed": true, "invocation": {"module_name": "ec2_key"}, "module_stderr": "  File \"~/.ansible/tmp/ansible-tmp-1538353060.27-179003914891970/ec2_key\", line 153\n    except Exception, e:\n                    ^\nSyntaxError: invalid syntax\n", "module_stdout": "", "msg": "MODULE FAILURE", "parsed": false}
# so, on a fresh ubuntu AWS instance, 
# aws ec2 run-instances --image-id ami-0789a5fb42dcccc10 --count 1 --instance-type t3.micro --key-name dev_staging --associate-public-ip-address --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=ansible_ubuntu}]' --region=ap-southeast-2
# I tried
# sudo apt-get update && sudo apt-get install software-properties-common && sudo apt-add-repository ppa:ansible/ansible && sudo apt-get update && sudo apt-get install ansible && sudo dpkg-reconfigure locales 
# locales was interactive... then
# sudo apt-get install python-pip && pip install boto3 && pip install --upgrade pip && pip3 install --user boto boto3
# then set up AWS 
# sudo apt install awscli && aws configure

# to set up Hugo
# curl -L https://github.com/gohugoio/hugo/releases/download/v0.49/hugo_0.49_Linux-64bit.deb -o hugo_0.49_Linux-64bit.deb
# sudo dpkg -i hugo_0.49_Linux-64bit.deb
# hugo new site IsentiaChallenge
# cd /home/ubuntu/IsentiaChallenge && git init && git submodule add https://github.com/cboettig/hugo-now-ui.git themes/now-ui && echo 'theme = "now-ui"' >> config.toml 




- hosts: localhost
  connection: local
  gather_facts: False
  vars:
    # ansible system variables
    ansible_host_key_checking: False  
    ansible_ssh_extra_args: "-o StrictHostKeyChecking=no"
    hugo_version: https://github.com/gohugoio/hugo/releases/download/v0.49/hugo_0.49_Linux-64bit.deb

    # AWS variables
    # (It would be better to get most of these by describing the account)
    local_aws_profile: default
    vpc_id: vpc-35a0f552
    ami_id: ami-0789a5fb42dcccc10   # ubuntu 16.04 LTS hvm:ebs-ssd  (t3 requires ebs root device)
    security_group_name: dev_staging_sg
    subnet_id1: subnet-a3ebd6c4   # surprise!  t3.micro is only available in 2a and 2c, not 2b.  without this we fail 1/3 of the time
    subnet_id2: subnet-f5e946ad
    region_name: ap-southeast-2
    instance_type: t2.micro # t3.micro isn't in the free tier
    target_group: IsentiaChallenge-tg

  tasks:
 
    - name: Install hugo
      apt: 
        deb: "{{ hugo_version }}"
      become: yes


    # and symlink it to somewhere that it can be found from cron
    - file:
        src: /usr/local/bin/hugo
        dest: /usr/bin/hugo
        state: link
      become: yes

    - name: Install fortune
      apt: 
        name: fortune
      state: present
      become: yes

    # and symlink it to somewhere that it can be found from cron
    - file:
        src: /usr/games/fortune
        dest: /usr/bin/fortune
        state: link
      become: yes

       
    - name: create a Key Pair 
      ec2_key:
        profile: "{{ local_aws_profile }}"
        name: "dev_staging"
        region: "{{ region_name }}"
        state: present
      register: dev_staging_ec2_key

    - name: Save private key
      copy: content="{{ dev_staging_ec2_key.key.private_key }}" dest="./aws/dev_staging.pem" mode=0600
      when: dev_staging_ec2_key.changed      

    - name: ec2 security group for Ansible / Hugin servers
      ec2_group:
        profile: "{{ local_aws_profile }}"
        name: "{{ security_group_name }}"
        region: "{{ region_name }}"
        description: SG for Ansible and Hugin
        vpc_id: "{{ vpc_id }}"
        rules:
        # SECURITY: we should have separate sg for elb and ec2, with elb allowing port 80 only and ec2 only available within the VPC
        - proto: tcp
          ports: 80
          cidr_ip: 0.0.0.0/0
        - proto: tcp
          ports: 81
          cidr_ip: 0.0.0.0/0
        - proto: tcp
          ports: 22
          cidr_ip: 60.242.164.83/32
          rule_desc: just andrew for the moment
        - proto: tcp
          ports: 22
          cidr_ip: 172.31.0.0/16
          rule_desc: all of this VPC 
        # TODO: gather facts uses the public IP.  Either work out how to make it use private IP or DNS; or allocate elastic IPs which we can know and control.  Until then, ssh has to be open to the world
        - proto: tcp
          ports: 22
          cidr_ip: 0.0.0.0/0
          rule_desc: allow ssh from everywhere


    # Create a pair of target groups with a default health check
    - elb_target_group:
        profile: "{{ local_aws_profile }}"
        name: "staging-{{ target_group }}"
        region: "{{ region_name }}"
        protocol: http
        port: 80
        vpc_id: "{{ vpc_id }}"
        state: present

    - elb_target_group:
        profile: "{{ local_aws_profile }}"
        name: "dev-{{ target_group }}"
        region: "{{ region_name }}"
        protocol: http
        port: 81
        vpc_id: "{{ vpc_id }}"
        state: present


    # Create a pair of ELBs for dev and staging
    - elb_application_lb:
        profile: "{{ local_aws_profile }}"
        name: staging-IsentiaChallenge-elb
        region: "{{ region_name }}"
        security_groups:
        - "{{ security_group_name }}"
        subnets:
        - "{{ subnet_id1 }}"
        - "{{ subnet_id2 }}"
        listeners:
        - Protocol: HTTP 
          Port: 80
          DefaultActions:
          - Type: forward # Required. Only 'forward' is accepted at this time
            TargetGroupName: "staging-{{ target_group }}"
        state: present          
              
    - elb_application_lb:
        profile: "{{ local_aws_profile }}"
        name: dev-IsentiaChallenge-elb
        region: "{{ region_name }}"
        security_groups:
        - "{{ security_group_name }}"
        subnets:
        - "{{ subnet_id1 }}"
        - "{{ subnet_id2 }}"
        listeners:
        - Protocol: HTTP 
          Port: 80
          DefaultActions:
          - Type: forward # Required. Only 'forward' is accepted at this time
            TargetGroupName: "dev-{{ target_group }}"
        state: present          
              
  
    - name: Provision an instance
      ec2:         
         profile: "{{ local_aws_profile }}"
#         key_name: "{{ dev_staging_ec2_key }"
         vpc_subnet_id: "{{ subnet_id1 }}"
         key_name: dev_staging
         region: "{{ region_name }}"
         group: "{{ security_group_name }}"
         instance_type: "{{ instance_type }}"
         image: "{{ ami_id }}"
         wait: true
         count: 1
         # SECURITY: public IP only for debugging
         assign_public_ip: yes
         instance_tags:
            Name: dev_staging         
         user_data: |
               #!/bin/sh
               # wait_for_connection fails if you haven't fixed the locale
               sudo locale-gen en_AU.UTF-8
               sudo apt-get update
               # gather_facts fails if python isn't installed on the target
               sudo apt-get install python -y
               # totally hacky way to get the ansible ping module to correctly find python
               sudo ln -s /usr/bin/python3 /usr/bin/python
#               sudo apt-get install nginx -y
      register: ec2
    
    - name: Add all instance public IPs to host group
      add_host: hostname={{ item.public_ip }} groups=ec2hosts 
      loop: "{{ ec2.instances }}"
     
# 
    - name: Wait for SSH to come up
      delegate_to: "{{ item.private_dns_name }}"
      wait_for_connection:
        delay: 60
        sleep: 20
        timeout: 600  # ten minutes because we're installing python
      with_items: "{{ ec2.instances }}"


    # add instance to target group / elb
    - elb_target:
        profile: "{{ local_aws_profile }}"
        region: "{{ region_name }}"
        target_group_name: "dev-{{ target_group }}"
        target_id: "{{ item.id }}"
        state: present
      with_items: "{{ ec2.instances }}"
      
    # add instance to target group / elb
    - elb_target:
        profile: "{{ local_aws_profile }}"
        region: "{{ region_name }}"
        target_group_name: "staging-{{ target_group }}"
        target_id: "{{ item.id }}"
        state: present
      with_items: "{{ ec2.instances }}"

# set up cron jobs (these might fail for the first couple of minutes until the host is ready)
    - name: push files to web server
      cron:
        name: "push to web server"
        minute: "1,6,11,16,21,26,31,36,41,46,51,56"
        hour: "*"
        job: "sleep 30 && AWS_PROFILE=default ansible-playbook -i ec2.py {{ playbook_dir }}/nginx.yml --key-file ~/.aws/dev_staging.pem  >> /var/log/IsentiaChallenge.log 2>&1"

    - name: add a new fortune
      cron:
        name: "add a new fortune"
        minute: "03,13,23,43,53"
        hour: "*"
        job: "python3 {{ playbook_dir }}/hugo.py dev  >> /var/log/IsentiaChallenge.log 2>&1"

    - name: version to staging
      cron:
        name: "version to staging"
        minute: "00"
        hour: "*"
        job: "python3 {{ playbook_dir }}/hugo.py staging  >> /var/log/IsentiaChallenge.log 2>&1"

      
- hosts: ec2hosts
  name: configuration play
  user: ubuntu
  gather_facts: true
  vars:
    ansible_ssh_extra_args: "-o StrictHostKeyChecking=no"

  tasks:

    - name: Install nginx
      apt: 
        name: nginx
      state: present
      become: yes
      
#    - name: Check nginx service
#      service: name=nginx state=started


     
    # this whole section is a funny mixture of copy-files and shell commands to create links etc.  it would make much more sense just to clone the whole /etc/nginx and /var/www from git

    # create vhost directories
    - file:
        path: /var/www/dev.isentia.andrewswebsite.net/html
        state: directory
        owner: root
        group: root
      become: yes

    - file:
        path: /var/www/staging.isentia.andrewswebsite.net/html
        state: directory
        owner: root
        group: root
      become: yes

     
    - name: copy dev nginx config and symlink
      copy:
        src: web-server/dev.isentia.andrewswebsite.net
        dest: /etc/nginx/sites-available/
        owner: root
        group: root
        mode: u=rw,g=r,o=r
      become: yes
     
    - name: copy staging nginx config and symlink
    # TODO: the permissions are really wrong.
      copy:
        src: web-server/staging.isentia.andrewswebsite.net
        dest: /etc/nginx/sites-available/
        owner: root
        group: root
        mode: u=rw,g=r,o=r
      become: yes

    - file:
        src: /etc/nginx/sites-available/staging.isentia.andrewswebsite.net
        dest: /etc/nginx/sites-enabled/staging.isentia.andrewswebsite.net
        owner: root
        group: root
        state: link
      become: yes

    - file:
        src: /etc/nginx/sites-available/dev.isentia.andrewswebsite.net
        dest: /etc/nginx/sites-enabled/dev.isentia.andrewswebsite.net
        owner: root
        group: root
        state: link
      become: yes
   
    - name: Restart nginx now that everything is set up
      service: 
        name: nginx 
        state: restarted
      become: yes



# tear down : 
# remove instance, remove keypair, 
